{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg-FdWorDFAA"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZLjMsE6edyHJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csvs from jupyter\n",
    "trainPayments = pd.read_csv('df_Payments.csv')\n",
    "trainOrderItems = pd.read_csv('df_OrderItems.csv')\n",
    "trainCustomers = pd.read_csv('df_Customers.csv')\n",
    "trainProducts = pd.read_csv('df_Products.csv')\n",
    "trainOrders = pd.read_csv('df_Orders.csv')\n",
    "\n",
    "testPayments = pd.read_csv('test_df_Payments.csv')\n",
    "testOrderItems = pd.read_csv('test_df_OrderItems.csv')\n",
    "testCustomers = pd.read_csv('test_df_Customers.csv')\n",
    "testProducts = pd.read_csv('test_df_Products.csv')\n",
    "testOrders = pd.read_csv('test_df_Orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Urp-0kGTeCcK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#connect google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\n#import csvs from google drive\\ntrainPayments = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Payments.csv')\\ntrainOrderItems = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_OrderItems.csv')\\ntrainCustomers = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Customers.csv')\\ntrainProducts = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Products.csv')\\ntrainOrders = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Orders.csv')\\n\\ntestPayments = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Payments.csv')\\ntestOrderItems = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_OrderItems.csv')\\ntestCustomers = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Customers.csv')\\ntestProducts = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Products.csv')\\ntestOrders = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Orders.csv')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#connect google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#import csvs from google drive\n",
    "trainPayments = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Payments.csv')\n",
    "trainOrderItems = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_OrderItems.csv')\n",
    "trainCustomers = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Customers.csv')\n",
    "trainProducts = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Products.csv')\n",
    "trainOrders = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/train/df_Orders.csv')\n",
    "\n",
    "testPayments = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Payments.csv')\n",
    "testOrderItems = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_OrderItems.csv')\n",
    "testCustomers = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Customers.csv')\n",
    "testProducts = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Products.csv')\n",
    "testOrders = pd.read_csv('/content/drive/MyDrive/Ecommerce_Order_Dataset/test/df_Orders.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aaejEzppkVUn"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([trainPayments, trainOrderItems, trainCustomers, trainProducts, trainOrders], axis=1) #merge training csvs to one dataframe\n",
    "df = df.T.drop_duplicates().T #get rid of duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZJfUNvvKoEKG"
   },
   "outputs": [],
   "source": [
    "test_df = pd.concat([testPayments, testOrderItems, testCustomers, testProducts, testOrders], axis=1) #merge testing csvs to one dataframe\n",
    "test_df = test_df.T.drop_duplicates().T #get rid of duplicate columns\n",
    "\n",
    "##Warning-- test_df does not have the last two columns of df, which give delivery time and delivery time prediction\n",
    "##Split df into training and testing datasets in order to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR6LaHVyWnvY"
   },
   "source": [
    "# Data cleaning/preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "riYLSUNprt7T"
   },
   "outputs": [],
   "source": [
    "#Get rid of unneeded data\n",
    "df = df.drop(columns=['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_zip_code_prefix', 'customer_city', 'order_status', 'order_approved_at', 'order_estimated_delivery_date'])\n",
    "test_df = test_df.drop(columns=['order_id', 'product_id', 'seller_id', 'customer_id', 'customer_zip_code_prefix', 'customer_city', 'order_approved_at', 'order_purchase_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hcx6JcPlplaZ"
   },
   "outputs": [],
   "source": [
    "#drops rows with NAN values\n",
    "df = df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EsurGvvur_y6"
   },
   "outputs": [],
   "source": [
    "# Replace payment types with a vector that represents a certain payment type\n",
    "payment_types = {'credit_card': 0, 'wallet': 1, 'voucher': 2, 'debit_card': 3}\n",
    "x0 = []\n",
    "for pay_type in list(df['payment_type']):\n",
    "  pay_type_vector = [0,0,0,0]\n",
    "  pay_type_vector[payment_types[pay_type]] = 1\n",
    "  x0.append(pay_type_vector)\n",
    "df['payment_type'] = x0\n",
    "\n",
    "xt0 = []\n",
    "for pay_type in list(test_df['payment_type']):\n",
    "  pay_type_vector = [0,0,0,0]\n",
    "  pay_type_vector[payment_types[pay_type]] = 1\n",
    "  xt0.append(pay_type_vector)\n",
    "test_df['payment_type'] = xt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_PjYrQ2nxlRU"
   },
   "outputs": [],
   "source": [
    "#Replace state with vector that represents state\n",
    "customer_state = {\n",
    "    'SP': 0, 'RJ': 1, 'MG': 2, 'SC': 3, 'ES': 4,\n",
    "    'RN': 5, 'BA': 6, 'DF': 7, 'RS': 8, 'PE': 9,\n",
    "    'GO': 10, 'CE': 11, 'PR': 12, 'MA': 13, 'PI': 14,\n",
    "    'MT': 15, 'MS': 16, 'SE': 17, 'RO': 18, 'TO': 19,\n",
    "    'AM': 20, 'AP': 21, 'PB': 22, 'PA': 23, 'AL': 24,\n",
    "    'AC': 25, 'RR': 26\n",
    "}\n",
    "x1 = []\n",
    "for state in list(df['customer_state']):\n",
    "  customer_state_vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "  customer_state_vector[customer_state[state]] = 1\n",
    "  x1.append(customer_state_vector)\n",
    "df['customer_state'] = x1\n",
    "\n",
    "xt1 = []\n",
    "for state in list(test_df['customer_state']):\n",
    "  customer_state_vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "  customer_state_vector[customer_state[state]] = 1\n",
    "  xt1.append(customer_state_vector)\n",
    "test_df['customer_state'] = xt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ycx62aM-L7HR"
   },
   "outputs": [],
   "source": [
    "# Put product categories into supercategories\n",
    "home_and_living = [\n",
    "    \"furniture_decor\",\n",
    "    \"bed_bath_table\",\n",
    "    \"kitchen_dining_laundry_garden_furniture\",\n",
    "    \"home_appliances\",\n",
    "    \"home_comfort\",\n",
    "    \"garden_tools\",\n",
    "    \"housewares\",\n",
    "    \"construction_tools_garden\",\n",
    "    \"construction_tools_construction\",\n",
    "    \"construction_tools_lights\",\n",
    "    \"construction_tools_safety\",\n",
    "    \"furniture_living_room\",\n",
    "    \"furniture_bedroom\",\n",
    "    \"furniture_mattress_and_upholstery\"\n",
    "]\n",
    "\n",
    "fashion_and_accessories = [\n",
    "    \"fashion_shoes\",\n",
    "    \"fashion_bags_accessories\",\n",
    "    \"fashion_male_clothing\",\n",
    "    \"fashio_female_clothing\",\n",
    "    \"fashion_childrens_clothes\",\n",
    "    \"fashion_underwear_beach\",\n",
    "    \"fashion_sport\"\n",
    "]\n",
    "\n",
    "toys_and_baby_products = [\n",
    "    \"toys\",\n",
    "    \"baby\",\n",
    "    \"diapers_and_hygiene\"\n",
    "]\n",
    "\n",
    "health_and_beauty = [\n",
    "    \"health_beauty\",\n",
    "    \"perfumery\"\n",
    "]\n",
    "\n",
    "electronics_and_gadgets = [\n",
    "    \"audio\",\n",
    "    \"computers_accessories\",\n",
    "    \"computers\",\n",
    "    \"consoles_games\",\n",
    "    \"telephony\",\n",
    "    \"small_appliances\",\n",
    "    \"tablets_printing_image\"\n",
    "]\n",
    "\n",
    "sports_and_leisure = [\n",
    "    \"sports_leisure\"\n",
    "]\n",
    "\n",
    "food_and_beverages = [\n",
    "    \"food\",\n",
    "    \"food_drink\",\n",
    "    \"drinks\"\n",
    "]\n",
    "\n",
    "arts_and_crafts = [\n",
    "    \"art\",\n",
    "    \"arts_and_craftmanship\"\n",
    "]\n",
    "\n",
    "gifts_and_celebrations = [\n",
    "    \"watches_gifts\",\n",
    "    \"party_supplies\",\n",
    "    \"christmas_supplies\"\n",
    "]\n",
    "\n",
    "pets_and_animals = [\n",
    "    \"pet_shop\"\n",
    "]\n",
    "\n",
    "office_and_stationery = [\n",
    "    \"office_furniture\",\n",
    "    \"stationery\"\n",
    "]\n",
    "\n",
    "industry_and_commerce = [\n",
    "    \"industry_commerce_and_business\",\n",
    "    \"signaling_and_security\",\n",
    "    \"agro_industry_and_commerce\"\n",
    "]\n",
    "\n",
    "media_and_entertainment = [\n",
    "    \"books_technical\",\n",
    "    \"books_general_interest\",\n",
    "    \"books_imported\",\n",
    "    \"dvds_blu_ray\",\n",
    "    \"cine_photo\",\n",
    "    \"music\"\n",
    "]\n",
    "\n",
    "misc = [\n",
    "    \"cool_stuff\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "cats0 = []\n",
    "\n",
    "for cat in list(df['product_category_name']):\n",
    "  if cat in home_and_living:\n",
    "    cat = 'home_and_living'\n",
    "  elif cat in fashion_and_accessories:\n",
    "    cat = 'fashion_and_accessories'\n",
    "  elif cat in toys_and_baby_products:\n",
    "    cat = 'toys_and_baby_products'\n",
    "  elif cat in health_and_beauty:\n",
    "    cat = 'health_and_beauty'\n",
    "  elif cat in electronics_and_gadgets:\n",
    "    cat = 'electronics_and_gadgets'\n",
    "  elif cat in sports_and_leisure:\n",
    "    cat = 'sports_and_leisure'\n",
    "  elif cat in food_and_beverages:\n",
    "    cat = 'food_and_beverages'\n",
    "  elif cat in arts_and_crafts:\n",
    "    cat = 'arts_and_crafts'\n",
    "  elif cat in gifts_and_celebrations:\n",
    "    cat = 'gifts_and_celebrations'\n",
    "  elif cat in pets_and_animals:\n",
    "    cat = 'pets_and_animals'\n",
    "  elif cat in office_and_stationery:\n",
    "    cat = 'office_and_stationery'\n",
    "  elif cat in industry_and_commerce:\n",
    "    cat = 'industry_and_commerce'\n",
    "  elif cat in media_and_entertainment:\n",
    "    cat = 'media_and_entertainment'\n",
    "  elif cat in misc:\n",
    "    cat = 'misc'\n",
    "  else:\n",
    "    cat = 'misc'\n",
    "  cats0.append(cat)\n",
    "\n",
    "df['product_category_name'] = cats0\n",
    "\n",
    "\n",
    "\n",
    "cats1 = []\n",
    "\n",
    "for cat in list(test_df['product_category_name']):\n",
    "  if cat in home_and_living:\n",
    "    cat = 'home_and_living'\n",
    "  elif cat in fashion_and_accessories:\n",
    "    cat = 'fashion_and_accessories'\n",
    "  elif cat in toys_and_baby_products:\n",
    "    cat = 'toys_and_baby_products'\n",
    "  elif cat in health_and_beauty:\n",
    "    cat = 'health_and_beauty'\n",
    "  elif cat in electronics_and_gadgets:\n",
    "    cat = 'electronics_and_gadgets'\n",
    "  elif cat in sports_and_leisure:\n",
    "    cat = 'sports_and_leisure'\n",
    "  elif cat in food_and_beverages:\n",
    "    cat = 'food_and_beverages'\n",
    "  elif cat in arts_and_crafts:\n",
    "    cat = 'arts_and_crafts'\n",
    "  elif cat in gifts_and_celebrations:\n",
    "    cat = 'gifts_and_celebrations'\n",
    "  elif cat in pets_and_animals:\n",
    "    cat = 'pets_and_animals'\n",
    "  elif cat in office_and_stationery:\n",
    "    cat = 'office_and_stationery'\n",
    "  elif cat in industry_and_commerce:\n",
    "    cat = 'industry_and_commerce'\n",
    "  elif cat in media_and_entertainment:\n",
    "    cat = 'media_and_entertainment'\n",
    "  elif cat in misc:\n",
    "    cat = 'misc'\n",
    "  else:\n",
    "    cat = 'misc'\n",
    "  cats1.append(cat)\n",
    "\n",
    "test_df['product_category_name'] = cats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h7Q436KaJrRq"
   },
   "outputs": [],
   "source": [
    "#Replace product category with vector representing the product category\n",
    "prod_cat = {\n",
    "    'home_and_living': 0,\n",
    "    'fashion_and_accessories': 1,\n",
    "    'toys_and_baby_products': 2,\n",
    "    'health_and_beauty': 3,\n",
    "    'electronics_and_gadgets': 4,\n",
    "    'sports_and_leisure': 5,\n",
    "    'food_and_beverages': 6,\n",
    "    'arts_and_crafts': 7,\n",
    "    'gifts_and_celebrations': 8,\n",
    "    'pets_and_animals': 9,\n",
    "    'office_and_stationery': 10,\n",
    "    'industry_and_commerce': 11,\n",
    "    'media_and_entertainment': 12,\n",
    "    'misc': 13\n",
    "}\n",
    "\n",
    "x2 = []\n",
    "for cat in list(df['product_category_name']):\n",
    "  prod_cat_vec = [0 for i in range(14)]\n",
    "  prod_cat_vec[prod_cat[cat]] = 1\n",
    "  x2.append(prod_cat_vec)\n",
    "df['product_category_name'] = x2\n",
    "\n",
    "xt2 = []\n",
    "for cat in list(test_df['product_category_name']):\n",
    "  prod_cat_vec = [0 for i in range(14)]\n",
    "  prod_cat_vec[prod_cat[cat]] = 1\n",
    "  xt2.append(prod_cat_vec)\n",
    "test_df['product_category_name'] = xt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QHxG95KP1wBq"
   },
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime format\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_delivered_timestamp'] = pd.to_datetime(df['order_delivered_timestamp'])\n",
    "\n",
    "# Find difference\n",
    "df['delivery_time'] = df['order_delivered_timestamp'] - df['order_purchase_timestamp']\n",
    "\n",
    "# Convert to hours\n",
    "df['delivery_time_hours'] = df['delivery_time'].dt.total_seconds() / 3600\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['order_purchase_timestamp', 'order_delivered_timestamp', 'delivery_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "vQIc5-GU2L-t",
    "outputId": "87fb2964-a97a-41f5-bab9-b716c842273f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35576\\893215237.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shipping_charges'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_category_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_weight_g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_length_cm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_height_cm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_width_cm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[1;32m-> 4062\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4063\u001b[0m         \u001b[0mcheck_dict_or_set_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4064\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4065\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Features and labels\n",
    "X = [] # Features\n",
    "y = [] # Label\n",
    "for i in range(len(df)):\n",
    "    y.append(list(df['delivery_time_hours'])[i])\n",
    "    \n",
    "    X.append(\n",
    "        [list(df['payment_sequential'])[i]] + \n",
    "        list(df['payment_type'])[i] +\n",
    "        [list(df['payment_installments'])[i]] + \n",
    "        [list(df['payment_value'])[i]] +\n",
    "        [list(df['price'])[i]] + \n",
    "        [list(df['shipping_charges'])[i]] +\n",
    "        list(df['customer_state'])[i] + \n",
    "        list(df['product_category_name'])[i] +\n",
    "        [list(df['product_weight_g'])[i]] + \n",
    "        [list(df['product_length_cm'])[i]] +\n",
    "        [list(df['product_height_cm'])[i]] + \n",
    "        [list(df['product_width_cm'])[i]]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPLhHsml4gQT"
   },
   "source": [
    "# Statistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "columns = ['Payment Sequential',\n",
    "    'Credit Card',\n",
    "    'Digital Wallet',\n",
    "    'Voucher',\n",
    "    'Debit Card',\n",
    "    'Payment Installments',\n",
    "    'Payment Value',\n",
    "    'Price',\n",
    "    'Shipping Charges',\n",
    "    'São Paulo (SP)',\n",
    "    'Rio de Janeiro (RJ)',\n",
    "    'Minas Gerais (MG)',\n",
    "    'Santa Catarina (SC)',\n",
    "    'Espírito Santo (ES)',\n",
    "    'Rio Grande do Norte (RN)',\n",
    "    'Bahia (BA)',\n",
    "    'Distrito Federal (DF)',\n",
    "    'Rio Grande do Sul (RS)',\n",
    "    'Pernambuco (PE)',\n",
    "    'Goiás (GO)',\n",
    "    'Ceará (CE)',\n",
    "    'Paraná (PR)',\n",
    "    'Maranhão (MA)',\n",
    "    'Piauí (PI)',\n",
    "    'Mato Grosso (MT)',\n",
    "    'Mato Grosso do Sul (MS)',\n",
    "    'Sergipe (SE)',\n",
    "    'Rondônia (RO)',\n",
    "    'Tocantins (TO)',\n",
    "    'Amazonas (AM)',\n",
    "    'Amapá (AP)',\n",
    "    'Paraíba (PB)',\n",
    "    'Pará (PA)',\n",
    "    'Alagoas (AL)',\n",
    "    'Acre (AC)',\n",
    "    'Roraima (RR)',\n",
    "    'Home and Living',\n",
    "    'Fashion and Accessories',\n",
    "    'Toys and Baby Products',\n",
    "    'Health and Beauty',\n",
    "    'Electronics and Gadgets',\n",
    "    'Sports and Leisure',\n",
    "    'Food and Beverages',\n",
    "    'Arts and Crafts',\n",
    "    'Gifts and Celebrations',\n",
    "    'Pets and Animals',\n",
    "    'Office and Stationery',\n",
    "    'Industry and Commerce',\n",
    "    'Media and Entertainment',\n",
    "    'Miscellaneous',\n",
    "    'Product Weight (g)',\n",
    "    'Product Length (cm)',\n",
    "    'Product Height (cm)',\n",
    "    'Product Width (cm)',\n",
    "    'Delivery Time (hours)'\n",
    "]\n",
    "\n",
    "data_corr = pd.DataFrame(np.hstack([X, np.expand_dims(y,-1)]), columns=columns).corr()\n",
    "\n",
    "labels = data_corr.copy()\n",
    "\n",
    "sns.heatmap(data_corr, annot=True, xticklabels=data_corr.columns, yticklabels=data_corr.columns)\n",
    "sns.set(rc = {'figure.figsize':(30,30)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(model, x_test, y_test):\n",
    "    num_errors = 0\n",
    "    tot_error = 0\n",
    "    for j in range(x_test):\n",
    "        tot_error += abs(model.predict(np.array([x_test[i]]))[0] - y_test[i])\n",
    "        num_errors += 1\n",
    "    return tot_error / num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR model\n",
    "error_df = {\"i\": [], \"Error\": []}\n",
    "error = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    svr = SVR(degree=i)\n",
    "    svr.fit(X_train_scaled, y_train)\n",
    "    avg_error = getError(svr, X_test, y_test);\n",
    "    error.append(avg_error)\n",
    "\n",
    "error_df[\"Error\"].append(error)\n",
    "pd.DataFrame(error_df)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBR model\n",
    "error_df = {\"i\": [], \"Error\": []}\n",
    "error = []\n",
    "\n",
    "for i in range(1, 11):    \n",
    "    gbr = GradientBoostingRegressor(n_estimators=i)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    avg_error = getError(gbr, X_test, y_test)\n",
    "    error.append(avg_error)\n",
    "\n",
    "error_df[\"Error\"].append(error)\n",
    "pd.DataFrame(error_df)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree regressor\n",
    "error_df = {\"i\": [], \"Error\": []}\n",
    "error = []\n",
    "\n",
    "for i in range(1, 11):    \n",
    "    regressor = DecisionTreeRegressor(max_depth=i)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    avg_error = getError(gbr, X_test, y_test)\n",
    "    error.append(avg_error)\n",
    "\n",
    "error_df[\"Error\"].append(error)\n",
    "pd.DataFrame(error_df)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest\n",
    "error_df = {\"i\": [], \"Error\": []}\n",
    "error = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    regressor = RandomForestRegressor(n_estimators=i)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    avg_error = getError(regressor, X_test, y_test)\n",
    "    error.append(avg_error)\n",
    "\n",
    "error_df[\"Error\"].append(error)\n",
    "pd.DataFrame(error_df)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network\n",
    "error_df = {\"i\": [], \"Error\": []}\n",
    "error = []\n",
    "\n",
    "def make_neural_network(num_hidden_layers):\n",
    "    layers = [Input(55)] #input layer with 55 dimensions\n",
    "    for i in range(num_hidden_layers): layers.append(Dense(100, activation=\"relu\"))\n",
    "    layers.append(Dense(1, activation=\"relu\")) #output layer with one neuron\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "for i in range(1,11):\n",
    "    model = make_neural_network(i)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=300, batch_size=100, verbose=0)\n",
    "    avg_error = getError(model, X_test, y_test)\n",
    "    error.append(avg_error)\n",
    "\n",
    "error_df[\"Error\"].append(error)\n",
    "pd.DataFrame(error_df)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
